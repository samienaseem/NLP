{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "happy-sigma",
   "metadata": {
    "id": "happy-sigma"
   },
   "source": [
    "# Lab 01 - Tokenisation and basic feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-supervision",
   "metadata": {
    "id": "downtown-supervision"
   },
   "source": [
    "This lab is an introduction to some basic test processing that will give us the first impression of the challenges in translating text into meaningful feature vector representations.\n",
    "\n",
    "So let's first set some text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stopped-throat",
   "metadata": {
    "id": "stopped-throat"
   },
   "outputs": [],
   "source": [
    "sentence = \"\"\" Welcome to the Natural Language Processing lab!\n",
    "               We'll learn many things in this no 1 lab, so we will take it easy.\n",
    "               Natural Language Processing is fun.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86574662",
   "metadata": {
    "id": "86574662"
   },
   "source": [
    "Let's also download some libraries we'll be using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aKciRQkx0Vze",
   "metadata": {
    "id": "aKciRQkx0Vze"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76028587",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76028587",
    "outputId": "8b33a989-1d49-4941-c6d7-b5203807ffc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/samar/anaconda3/lib/python3.11/site-packages (1.25.2)\n",
      "Requirement already satisfied: pandas in /Users/samar/anaconda3/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/samar/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: nltk in /Users/samar/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/samar/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/samar/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/samar/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/samar/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/samar/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/samar/anaconda3/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: click in /Users/samar/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/samar/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/samar/anaconda3/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/samar/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da2c66",
   "metadata": {
    "id": "28da2c66"
   },
   "source": [
    "## Using native Python functions for tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-perry",
   "metadata": {
    "id": "southwest-perry"
   },
   "source": [
    "Let's try to split the text based on spaces using the built in string function `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "careful-cleaning",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "careful-cleaning",
    "outputId": "d9c3129f-2814-4205-bc76-ffc6424ede22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab!',\n",
       " \"We'll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab,',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-brain",
   "metadata": {
    "id": "mounted-brain"
   },
   "source": [
    "You will observe that some words are not well separated from punctuation and contain some appended onto them.\n",
    "So we need to find a way to remove those characters... but, before we do that, let's see how we can create a quick feature vector first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regional-services",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "regional-services",
    "outputId": "96506f22-3267-49ea-848f-5f3ac2ffe05a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'Language',\n",
       " 'Natural',\n",
       " 'Processing',\n",
       " \"We'll\",\n",
       " 'Welcome',\n",
       " 'easy.',\n",
       " 'fun.',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'lab!',\n",
       " 'lab,',\n",
       " 'learn',\n",
       " 'many',\n",
       " 'no',\n",
       " 'so',\n",
       " 'take',\n",
       " 'the',\n",
       " 'things',\n",
       " 'this',\n",
       " 'to',\n",
       " 'we',\n",
       " 'will']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sentence.split()  # splitting based on spaces\n",
    "vocab = sorted(set(tokens))  # sorting and removing duplicates by using set()\n",
    "vocab  # just printing the vocab so we can look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-syndication",
   "metadata": {
    "id": "hundred-syndication"
   },
   "source": [
    "We can see that the sorted list has the numbers first, followed by capital and then lower case letters (all alphabetically sorted). We also see that repeating words appear only once in our vocabulary list. Let's compare the size of the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "periodic-typing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "periodic-typing",
    "outputId": "0a4b53a0-9ef0-4b86-fe2d-0a22b19dbe95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 27\n",
      "Vocab: 24\n"
     ]
    }
   ],
   "source": [
    "tokens_len = len(tokens)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "print(f\"Tokens: {tokens_len}\")\n",
    "print(f\"Vocab: {vocab_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-private",
   "metadata": {
    "id": "published-private"
   },
   "source": [
    "Let's try and print the matrix of tokens against vocabulary. We will use the numpy lib for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ultimate-litigation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ultimate-litigation",
    "outputId": "beaf4516-8cd4-4277-8d21-eff58da3a684"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.zeros((tokens_len, vocab_len), int)\n",
    "for i, token in enumerate(tokens):\n",
    "    matrix[i, vocab.index(token)] = 1\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-beach",
   "metadata": {
    "id": "designed-beach"
   },
   "source": [
    "It's not easy to see, but the second, third and fourth columns have the value of 1 in two rows, whereas the rest only take the value of 1 in a single row. To make it a little more readable, we could use Pandas and DataFrame! Both Pandas and NumPy are very useful libs that we will use many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "realistic-wallace",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "realistic-wallace",
    "outputId": "31ba3188-1fe2-4349-e150-1474c98e4a07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>Language</th>\n",
       "      <th>Natural</th>\n",
       "      <th>Processing</th>\n",
       "      <th>We'll</th>\n",
       "      <th>Welcome</th>\n",
       "      <th>easy.</th>\n",
       "      <th>fun.</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>...</th>\n",
       "      <th>many</th>\n",
       "      <th>no</th>\n",
       "      <th>so</th>\n",
       "      <th>take</th>\n",
       "      <th>the</th>\n",
       "      <th>things</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1  Language  Natural  Processing  We'll  Welcome  easy.  fun.  in  is  \\\n",
       "0   0         0        0           0      0        1      0     0   0   0   \n",
       "1   0         0        0           0      0        0      0     0   0   0   \n",
       "2   0         0        0           0      0        0      0     0   0   0   \n",
       "3   0         0        1           0      0        0      0     0   0   0   \n",
       "4   0         1        0           0      0        0      0     0   0   0   \n",
       "5   0         0        0           1      0        0      0     0   0   0   \n",
       "6   0         0        0           0      0        0      0     0   0   0   \n",
       "7   0         0        0           0      1        0      0     0   0   0   \n",
       "8   0         0        0           0      0        0      0     0   0   0   \n",
       "9   0         0        0           0      0        0      0     0   0   0   \n",
       "10  0         0        0           0      0        0      0     0   0   0   \n",
       "11  0         0        0           0      0        0      0     0   1   0   \n",
       "12  0         0        0           0      0        0      0     0   0   0   \n",
       "13  0         0        0           0      0        0      0     0   0   0   \n",
       "14  1         0        0           0      0        0      0     0   0   0   \n",
       "15  0         0        0           0      0        0      0     0   0   0   \n",
       "16  0         0        0           0      0        0      0     0   0   0   \n",
       "17  0         0        0           0      0        0      0     0   0   0   \n",
       "18  0         0        0           0      0        0      0     0   0   0   \n",
       "19  0         0        0           0      0        0      0     0   0   0   \n",
       "20  0         0        0           0      0        0      0     0   0   0   \n",
       "21  0         0        0           0      0        0      1     0   0   0   \n",
       "22  0         0        1           0      0        0      0     0   0   0   \n",
       "23  0         1        0           0      0        0      0     0   0   0   \n",
       "24  0         0        0           1      0        0      0     0   0   0   \n",
       "25  0         0        0           0      0        0      0     0   0   1   \n",
       "26  0         0        0           0      0        0      0     1   0   0   \n",
       "\n",
       "    ...  many  no  so  take  the  things  this  to  we  will  \n",
       "0   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "1   ...     0   0   0     0    0       0     0   1   0     0  \n",
       "2   ...     0   0   0     0    1       0     0   0   0     0  \n",
       "3   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "4   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "5   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "6   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "7   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "8   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "9   ...     1   0   0     0    0       0     0   0   0     0  \n",
       "10  ...     0   0   0     0    0       1     0   0   0     0  \n",
       "11  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "12  ...     0   0   0     0    0       0     1   0   0     0  \n",
       "13  ...     0   1   0     0    0       0     0   0   0     0  \n",
       "14  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "15  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "16  ...     0   0   1     0    0       0     0   0   0     0  \n",
       "17  ...     0   0   0     0    0       0     0   0   1     0  \n",
       "18  ...     0   0   0     0    0       0     0   0   0     1  \n",
       "19  ...     0   0   0     1    0       0     0   0   0     0  \n",
       "20  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "21  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "22  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "23  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "24  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "25  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "26  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "\n",
       "[27 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-barrel",
   "metadata": {
    "id": "increasing-barrel"
   },
   "source": [
    "Now this is a lot more clear and if we wanted we could carry on making it look nicer.\n",
    "\n",
    "Let's now carry on building the bag of words (BoW) -\n",
    "- *Bags of words (BoW) are a basic representation technique used in NLP.*\n",
    "- *Its main purpose is to convert textual data into a numerical format that can be used for further analysis.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anonymous-anaheim",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "anonymous-anaheim",
    "outputId": "f2ab60c0-016a-4d9f-b23b-e6ae2929ab21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1),\n",
       " ('Language', 1),\n",
       " ('Natural', 1),\n",
       " ('Processing', 1),\n",
       " (\"We'll\", 1),\n",
       " ('Welcome', 1),\n",
       " ('easy.', 1),\n",
       " ('fun.', 1),\n",
       " ('in', 1),\n",
       " ('is', 1),\n",
       " ('it', 1),\n",
       " ('lab!', 1),\n",
       " ('lab,', 1),\n",
       " ('learn', 1),\n",
       " ('many', 1),\n",
       " ('no', 1),\n",
       " ('so', 1),\n",
       " ('take', 1),\n",
       " ('the', 1),\n",
       " ('things', 1),\n",
       " ('this', 1),\n",
       " ('to', 1),\n",
       " ('we', 1),\n",
       " ('will', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = {token: 1 for token in tokens}  # setting this up as a dictionary\n",
    "sorted(bow.items())  # lets print it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-palestine",
   "metadata": {
    "id": "better-palestine"
   },
   "source": [
    "Since bow is a dictionary, we see that there are no duplicate words.\n",
    "\n",
    "Pandas also has a more efficient form of a dictionary called `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adequate-aquatic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "adequate-aquatic",
    "outputId": "e1b69c4b-a32b-4567-fa69-7f26dc14d934"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Welcome</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "      <th>Natural</th>\n",
       "      <th>Language</th>\n",
       "      <th>Processing</th>\n",
       "      <th>lab!</th>\n",
       "      <th>We'll</th>\n",
       "      <th>learn</th>\n",
       "      <th>many</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>lab,</th>\n",
       "      <th>so</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>take</th>\n",
       "      <th>it</th>\n",
       "      <th>easy.</th>\n",
       "      <th>is</th>\n",
       "      <th>fun.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Welcome  to  the  Natural  Language  Processing  lab!  We'll  learn  \\\n",
       "sent        1   1    1        1         1           1     1      1      1   \n",
       "\n",
       "      many  ...  1  lab,  so  we  will  take  it  easy.  is  fun.  \n",
       "sent     1  ...  1     1   1   1     1     1   1      1   1     1  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in tokens])), columns=[\"sent\"]).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "senior-shareware",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "senior-shareware",
    "outputId": "3f40f941-843d-480a-90a1-331fe9c4dc66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Welcome</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "      <th>Natural</th>\n",
       "      <th>Language</th>\n",
       "      <th>Processing</th>\n",
       "      <th>lab!</th>\n",
       "      <th>We'll</th>\n",
       "      <th>learn</th>\n",
       "      <th>many</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>lab,</th>\n",
       "      <th>so</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>take</th>\n",
       "      <th>it</th>\n",
       "      <th>easy.</th>\n",
       "      <th>is</th>\n",
       "      <th>fun.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Welcome  to  the  Natural  Language  Processing  lab!  We'll  learn  \\\n",
       "sent0        1   1    1        1         1           1     1      0      0   \n",
       "sent1        0   0    0        0         0           0     0      1      1   \n",
       "sent2        0   0    0        1         1           1     0      0      0   \n",
       "\n",
       "       many  ...  1  lab,  so  we  will  take  it  easy.  is  fun.  \n",
       "sent0     0  ...  0     0   0   0     0     0   0      0   0     0  \n",
       "sent1     1  ...  1     1   1   1     1     1   1      1   0     0  \n",
       "sent2     0  ...  0     0   0   0     0     0   0      0   1     1  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {}\n",
    "for i, sent in enumerate(sentence.split('\\n')):\n",
    "    corpus[f\"sent{i}\"] = dict((tok, 1) for tok in sent.split())\n",
    "\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "362cb5c0-a2a8-4a29-befd-d54d0d24b420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Welcome': 1,\n",
       " 'to': 1,\n",
       " 'the': 1,\n",
       " 'Natural': 1,\n",
       " 'Language': 1,\n",
       " 'Processing': 1,\n",
       " 'lab!': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['sent0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c090830c-11e0-4ad0-adc4-03c59a556852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"We'll\": 1,\n",
       " 'learn': 1,\n",
       " 'many': 1,\n",
       " 'things': 1,\n",
       " 'in': 1,\n",
       " 'this': 1,\n",
       " 'no': 1,\n",
       " '1': 1,\n",
       " 'lab,': 1,\n",
       " 'so': 1,\n",
       " 'we': 1,\n",
       " 'will': 1,\n",
       " 'take': 1,\n",
       " 'it': 1,\n",
       " 'easy.': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['sent1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-rachel",
   "metadata": {
    "id": "every-rachel"
   },
   "source": [
    "Now we see how we managed to build feature vectors for the three sentences we originally had. Now let's do a Dot Product calculation.\n",
    "\n",
    "- *In the context of measuring similarity, the dot product is often used to quantify the **similarity** between two vectors.*\n",
    "- *The **higher** the dot product, the more **similar** the vectors are considered to be.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6458b72-34d5-4613-9774-eb310285ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Natural': 1, 'Language': 1, 'Processing': 1, 'is': 1, 'fun.': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['sent2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "computational-tyler",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "computational-tyler",
    "outputId": "c9121871-ec41-4439-ace7-55fe481afedd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of sent0 from sent1: 0 and dot product of sent0 from sent2: 3\n"
     ]
    }
   ],
   "source": [
    "df = df.T\n",
    "print(f\"Dot product of sent0 from sent1: {df.sent0.dot(df.sent1)} and dot product of sent0 from sent2: {df.sent0.dot(df.sent2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-pavilion",
   "metadata": {
    "id": "vocational-pavilion"
   },
   "source": [
    "As we see from the results, the higher the dot product to more similar the vectors are... so given that only the first and last sentence have some common words, we see that this comes back as 3, where as the two sentences who have nothing in common come bak as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-realtor",
   "metadata": {
    "id": "arabic-realtor"
   },
   "source": [
    "We can improve our vocabulary now if we were to remove all other punctuation. Let's do that with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "saving-fifty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saving-fifty",
    "outputId": "0dd7dd9e-2f4e-49c3-d0d3-7d6a781949d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab',\n",
       " \"We'll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(r\"[-\\s.,;!?]+\", sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710aa204",
   "metadata": {
    "id": "710aa204"
   },
   "source": [
    "## NLTK\n",
    "\n",
    "Although this seems to be great... you might still have issues with different characters that are not anticipated. So we usually use an existing NLP related tokeniser to do this job. Let's try the NLTK lib.\n",
    "\n",
    "NLTK also supports regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prime-limit",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prime-limit",
    "outputId": "2562cc7f-6080-4546-a9fb-52af9482421d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab',\n",
       " '!',\n",
       " 'We',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy',\n",
       " '.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|$[0-9.]+|\\S+\")\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-prototype",
   "metadata": {
    "id": "handmade-prototype"
   },
   "source": [
    "but there are other more specialised tokenisers, such as the *Treebank Word Tokenizer*:\n",
    "\n",
    "\n",
    "- *It is specifically designed to tokenize text in a manner similar to the **Penn Treebank Tokenization conventions**.*\n",
    "- *The Penn Treebank is a **large corpus** of English text, and the tokenization conventions used in its processing have become a standard for various NLP tasks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rapid-dictionary",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rapid-dictionary",
    "outputId": "bdcbcac3-9fbe-4567-9682-65233e032054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab',\n",
       " '!',\n",
       " 'We',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-graphic",
   "metadata": {
    "id": "adjusted-graphic"
   },
   "source": [
    "For now let's use the regular expression special word pattern `\\w`, so we can control what the tokeniser does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-engine",
   "metadata": {
    "id": "south-engine",
    "outputId": "12ff114e-2f8a-4d06-d51e-dcbabd74b709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'to', 'the', 'Natural', 'Language', 'Processing', 'lab', 'We', 'll', 'learn', 'many', 'things', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'Natural', 'Language', 'Processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "#\\w is a special sequence in regular expressions that matches any alphanumeric character, including letters (both uppercase and lowercase) and digits.\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-category",
   "metadata": {
    "id": "loved-category"
   },
   "source": [
    "At the point you could try out different other tokenisers from other libraries and see if there are any differences.\n",
    "\n",
    "We will now calculate the 2-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-export",
   "metadata": {
    "id": "guilty-export",
    "outputId": "3a0e4a78-aade-4ddc-9762-70399f02e81e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Welcome', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'Natural'),\n",
       " ('Natural', 'Language'),\n",
       " ('Language', 'Processing'),\n",
       " ('Processing', 'lab'),\n",
       " ('lab', 'We'),\n",
       " ('We', 'll'),\n",
       " ('ll', 'learn'),\n",
       " ('learn', 'many'),\n",
       " ('many', 'things'),\n",
       " ('things', 'in'),\n",
       " ('in', 'this'),\n",
       " ('this', 'no'),\n",
       " ('no', '1'),\n",
       " ('1', 'lab'),\n",
       " ('lab', 'so'),\n",
       " ('so', 'we'),\n",
       " ('we', 'will'),\n",
       " ('will', 'take'),\n",
       " ('take', 'it'),\n",
       " ('it', 'easy'),\n",
       " ('easy', 'Natural'),\n",
       " ('Natural', 'Language'),\n",
       " ('Language', 'Processing'),\n",
       " ('Processing', 'is'),\n",
       " ('is', 'fun')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-present",
   "metadata": {
    "id": "connected-present"
   },
   "source": [
    "and 3-grams:\n",
    "\n",
    "These *$n$-grams* are a contiguous sequence of $n$ items from a given sample of text or speech. They can be used to **identify** common phrases or expressions and can be valuable in tasks like language modeling, where predicting the next word is based on the preceding one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-retreat",
   "metadata": {
    "id": "imperial-retreat",
    "outputId": "ccc644f5-7f28-4c25-dd42-bd085b71e297"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Welcome', 'to', 'the'),\n",
       " ('to', 'the', 'Natural'),\n",
       " ('the', 'Natural', 'Language'),\n",
       " ('Natural', 'Language', 'Processing'),\n",
       " ('Language', 'Processing', 'lab'),\n",
       " ('Processing', 'lab', 'We'),\n",
       " ('lab', 'We', 'll'),\n",
       " ('We', 'll', 'learn'),\n",
       " ('ll', 'learn', 'many'),\n",
       " ('learn', 'many', 'things'),\n",
       " ('many', 'things', 'in'),\n",
       " ('things', 'in', 'this'),\n",
       " ('in', 'this', 'no'),\n",
       " ('this', 'no', '1'),\n",
       " ('no', '1', 'lab'),\n",
       " ('1', 'lab', 'so'),\n",
       " ('lab', 'so', 'we'),\n",
       " ('so', 'we', 'will'),\n",
       " ('we', 'will', 'take'),\n",
       " ('will', 'take', 'it'),\n",
       " ('take', 'it', 'easy'),\n",
       " ('it', 'easy', 'Natural'),\n",
       " ('easy', 'Natural', 'Language'),\n",
       " ('Natural', 'Language', 'Processing'),\n",
       " ('Language', 'Processing', 'is'),\n",
       " ('Processing', 'is', 'fun')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-shooting",
   "metadata": {
    "id": "aggregate-shooting"
   },
   "source": [
    "If we want to include the n-grams as strings rather than tuples, then we need to convert them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-enhancement",
   "metadata": {
    "id": "twenty-enhancement",
    "outputId": "94a2584b-2875-4a10-e45c-ab5b2030571b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: ['Welcome to', 'to the', 'the Natural', 'Natural Language', 'Language Processing', 'Processing lab', 'lab We', 'We ll', 'll learn', 'learn many', 'many things', 'things in', 'in this', 'this no', 'no 1', '1 lab', 'lab so', 'so we', 'we will', 'will take', 'take it', 'it easy', 'easy Natural', 'Natural Language', 'Language Processing', 'Processing is', 'is fun']\n",
      "\n",
      "Trigrams: ['Welcome to the', 'to the Natural', 'the Natural Language', 'Natural Language Processing', 'Language Processing lab', 'Processing lab We', 'lab We ll', 'We ll learn', 'll learn many', 'learn many things', 'many things in', 'things in this', 'in this no', 'this no 1', 'no 1 lab', '1 lab so', 'lab so we', 'so we will', 'we will take', 'will take it', 'take it easy', 'it easy Natural', 'easy Natural Language', 'Natural Language Processing', 'Language Processing is', 'Processing is fun']\n"
     ]
    }
   ],
   "source": [
    "bigrams = [\" \".join(x) for x in list(ngrams(tokens, 2))]\n",
    "print(f\"Bigrams: {bigrams}\\n\")\n",
    "\n",
    "trigrams = [\" \".join(x) for x in list(ngrams(tokens, 3))]\n",
    "print(f\"Trigrams: {trigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-syracuse",
   "metadata": {
    "id": "seven-syracuse"
   },
   "source": [
    "Another important step we looked at in the lectures are the stop words. Let's try to use the nltk stop word list to remove them.\n",
    "\n",
    "These are words that are *commonly used* in a language but are typically *filtered out* during text processing because they are considered to carry little to no meaning on their own.\n",
    "\n",
    "First, let's download the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "apparent-going",
   "metadata": {
    "id": "apparent-going",
    "outputId": "1df273c0-ee79-4814-f2d9-dde77b8f3c3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/samar/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-disorder",
   "metadata": {
    "id": "annual-disorder"
   },
   "source": [
    "and now check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pending-reducing",
   "metadata": {
    "id": "pending-reducing",
    "outputId": "8df3e6c6-fa3f-40ef-dc16-6e1d7335fe52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(f\"number of stopwords: {len(stop_words)}\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-expression",
   "metadata": {
    "id": "friendly-expression"
   },
   "source": [
    "Other libs have different stopwords. Let's see a much larger set from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "monthly-armenia",
   "metadata": {
    "id": "monthly-armenia",
    "outputId": "199bca11-3c7d-4b97-fb9b-0c1d8bdc2c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 318\n",
      "frozenset({'before', 'anything', 'the', 'whereafter', 'back', 'thence', 'by', 'everywhere', 'onto', 'or', 'less', 'part', 'each', 'after', 'itself', 'ours', 'because', 'see', 'ever', 'due', 'formerly', 'amount', 'everything', 'except', 'on', 'do', 'fifteen', 'in', 'and', 'cry', 'two', 'name', 'mine', 'whereby', 'must', 'wherein', 'anyone', 'above', 'couldnt', 'system', 'i', 're', 'least', 'whoever', 'afterwards', 'several', 'seeming', 'that', 'themselves', 'beside', 'eight', 'than', 'mostly', 'hence', 'although', 'hereupon', 'becomes', 'found', 'give', 'being', 'some', 'as', 'much', 'without', 'now', 'own', 'somehow', 'upon', 'though', 'if', 'something', 'during', 'from', 'ourselves', 'ie', 'already', 'at', 'fire', 'full', 'had', 'throughout', 'put', 'once', 'whose', 'yourselves', 'up', 'sometimes', 'eleven', 'to', 'into', 'him', 'her', 'almost', 'five', 'those', 'first', 'we', 'have', 'can', 'across', 'whereas', 'hasnt', 'yourself', 'any', 'via', 'none', 'here', 'too', 'wherever', 'hereby', 'twelve', 'which', 'also', 'therefore', 'fifty', 'between', 'same', 'hundred', 'con', 'seems', 'only', 'seem', 'while', 'inc', 'front', 'seemed', 'elsewhere', 'detail', 'no', 'these', 'myself', 'show', 'alone', 'be', 'became', 'when', 'thus', 'has', 'nevertheless', 'down', 'therein', 'rather', 'interest', 'yours', 'about', 'serious', 'besides', 'off', 'yet', 'again', 'what', 'am', 'out', 'indeed', 'latterly', 'thick', 'will', 'ten', 'among', 'thin', 'go', 'ltd', 'not', 'whither', 'its', 'my', 'cannot', 'me', 'often', 'former', 'whether', 'below', 'might', 'against', 'become', 'under', 'bottom', 'over', 'one', 'next', 'there', 'thru', 'why', 'nothing', 'otherwise', 'enough', 'further', 'done', 'his', 'within', 'whenever', 'everyone', 'made', 'nine', 'whole', 'behind', 'been', 'other', 'is', 'neither', 'so', 'who', 'another', 'until', 'sincere', 'towards', 'anywhere', 'they', 'both', 'keep', 'herself', 'beyond', 'himself', 'together', 'through', 'nobody', 'he', 'along', 'our', 'amoungst', 'whatever', 'others', 'fill', 'an', 'where', 'she', 'thereby', 'noone', 'how', 'still', 'please', 'hereafter', 'then', 'third', 'latter', 'nor', 'mill', 'three', 'anyway', 'per', 'it', 'could', 'many', 'either', 'around', 'call', 'every', 'becoming', 'for', 'forty', 'else', 'beforehand', 'such', 'whence', 'whom', 'always', 'them', 'co', 'most', 'their', 'twenty', 'but', 'however', 'sixty', 'get', 'hers', 'namely', 'more', 'never', 'would', 'should', 'very', 'all', 'six', 'was', 'thereupon', 'herein', 'were', 'few', 'us', 'last', 'whereupon', 'sometime', 'take', 'a', 'may', 'toward', 'even', 'de', 'describe', 'with', 'bill', 'since', 'moreover', 'this', 'four', 'someone', 'well', 'anyhow', 'amongst', 'of', 'meanwhile', 'cant', 'perhaps', 'somewhere', 'thereafter', 'un', 'move', 'etc', 'find', 'your', 'top', 'eg', 'are', 'nowhere', 'you', 'side', 'empty'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "\n",
    "print(f\"number of stopwords: {len(sklearn_stop_words)}\")\n",
    "print(sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-deviation",
   "metadata": {
    "id": "exempt-deviation"
   },
   "source": [
    "Strangely enough, although there are more stop words in sklearn, you will find that nltk has words that are not contained in sklearn. So you might want to join the two lists.\n",
    "\n",
    "For normalising the text you could do something as simple as making sure all words are lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-employee",
   "metadata": {
    "id": "alive-employee",
    "outputId": "9d01852c-4672-43b3-8b06-85e0dbfc019d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'processing', 'lab', 'we', 'll', 'learn', 'many', 'things', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'natural', 'language', 'processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "norm_tokens = [x.lower() for x in tokens]\n",
    "print(norm_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-inflation",
   "metadata": {
    "id": "ongoing-inflation"
   },
   "source": [
    "For stemming the words we could use nltk again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-reputation",
   "metadata": {
    "id": "upset-reputation",
    "outputId": "ef21a6e6-975c-41d0-c963-40efefdccc72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcom', 'to', 'the', 'natur', 'languag', 'process', 'lab', 'we', 'll', 'learn', 'mani', 'thing', 'in', 'thi', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easi', 'natur', 'languag', 'process', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem_tokens = [stemmer.stem(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-porter",
   "metadata": {
    "id": "gothic-porter"
   },
   "source": [
    "For lemmatising, nltk also does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-diploma",
   "metadata": {
    "id": "desperate-diploma",
    "outputId": "387feb38-bdda-4316-b6ac-6818e9bf0318"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'processing', 'lab', 'we', 'll', 'learn', 'many', 'thing', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'natural', 'language', 'processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stem_tokens = [lemmatizer.lemmatize(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-relevance",
   "metadata": {
    "id": "coordinated-relevance"
   },
   "source": [
    "With this example sentence, there are no issues with the lemmatisation... but let's look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-veteran",
   "metadata": {
    "id": "understanding-veteran",
    "outputId": "0d772e42-9bdd-4984-9586-8c605d2e8c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", \"a\"))  # declaring the POS as adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-martin",
   "metadata": {
    "id": "intimate-martin"
   },
   "source": [
    "If we don't include the Part-of-speech (POS), nltk, using wordnet, does not work well. So let's try to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-merit",
   "metadata": {
    "id": "russian-merit"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map the POS tag to the first character lemmatize() accepts.\"\"\"\n",
    "\n",
    "    try:  # download nltk's POS tagger if it doesn't exist\n",
    "        nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()  # use ntlk's POS tagger on the word\n",
    "\n",
    "    # now we need to convert from nltk to wordnet POS notations (for compatibility reasons)\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # return and default to noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-comparative",
   "metadata": {
    "id": "imperial-comparative",
    "outputId": "c90379a0-69fe-4f29-96d1-c8f961eb00f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'processing', 'lab', 'we', 'll', 'learn', 'many', 'thing', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'natural', 'language', 'processing', 'be', 'fun']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stem_tokens = [lemmatizer.lemmatize(x, pos=get_wordnet_pos(x)) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-assignment",
   "metadata": {
    "id": "inside-assignment"
   },
   "source": [
    "If we look at the words now we are getting more counts for our bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-simon",
   "metadata": {
    "id": "polished-simon",
    "outputId": "9a2cb671-e98a-4735-e883-23c5a9575d3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'welcome': 1,\n",
       "         'to': 1,\n",
       "         'the': 1,\n",
       "         'natural': 2,\n",
       "         'language': 2,\n",
       "         'processing': 2,\n",
       "         'lab': 2,\n",
       "         'we': 2,\n",
       "         'll': 1,\n",
       "         'learn': 1,\n",
       "         'many': 1,\n",
       "         'thing': 1,\n",
       "         'in': 1,\n",
       "         'this': 1,\n",
       "         'no': 1,\n",
       "         '1': 1,\n",
       "         'so': 1,\n",
       "         'will': 1,\n",
       "         'take': 1,\n",
       "         'it': 1,\n",
       "         'easy': 1,\n",
       "         'be': 1,\n",
       "         'fun': 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bow = Counter(stem_tokens)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-healthcare",
   "metadata": {
    "id": "optional-healthcare"
   },
   "source": [
    "Now let's check the most frequent 6 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-champagne",
   "metadata": {
    "id": "unable-champagne",
    "outputId": "dd98e84c-5614-483c-df75-c43a86dc75a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural', 2),\n",
       " ('language', 2),\n",
       " ('processing', 2),\n",
       " ('lab', 2),\n",
       " ('we', 2),\n",
       " ('welcome', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.most_common(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-hayes",
   "metadata": {
    "id": "cross-hayes"
   },
   "source": [
    "Now let's remove the stop words and check the count again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-showcase",
   "metadata": {
    "id": "graduate-showcase",
    "outputId": "bface5c9-228a-44b0-9ca6-bf34c23a81a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'welcome': 1,\n",
       "         'natural': 2,\n",
       "         'language': 2,\n",
       "         'processing': 2,\n",
       "         'lab': 2,\n",
       "         'learn': 1,\n",
       "         'many': 1,\n",
       "         'thing': 1,\n",
       "         '1': 1,\n",
       "         'take': 1,\n",
       "         'easy': 1,\n",
       "         'fun': 1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_tokens = [x for x in stem_tokens if x not in stop_words]\n",
    "count = Counter(no_stop_tokens)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-favor",
   "metadata": {
    "id": "discrete-favor"
   },
   "source": [
    "Finally... let's make our feature vector using the frequency ratio (term count / total number of terms in the doc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-medicine",
   "metadata": {
    "id": "retained-medicine",
    "outputId": "eaae2f23-99eb-491f-b593-dbc158139578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125, 0.125, 0.125, 0.125, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]\n"
     ]
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(no_stop_tokens)\n",
    "for key, value in count.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "print(document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-spell",
   "metadata": {
    "id": "forced-spell"
   },
   "source": [
    "We have explored many many options already and we will continue with more advanced feature vectors in the next lab, plus some visualisations in charts. So until then please try different experiments on your own:\n",
    "1. See if you can change the text and have more sentences with different topics (so you can compare the feature vectors later)\n",
    "2. Try to use different libraries for tokenising, PoS tagging, stemming and lemmatising.\n",
    "3. Try to use distance metrics to compare vectors, such as Euclidian distance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
